{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DongUk-Park/RnD/blob/main/POI/NAIS/NAIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmJe4YiGrdB4"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7ISab2XUBWu"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "git_test\n"
          ]
        }
      ],
      "source": [
        "print(\"git_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHtVftt-mF8l"
      },
      "source": [
        "## Data Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlCC-gvCTdPE"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "  train_dataset_path = '/content/history_train.csv'\n",
        "  validation_dataset_path = '/content/history_validation.csv'\n",
        "  test_dataset_path = '/content/history_test.csv'\n",
        "\n",
        "  train_history_data = []\n",
        "  validation_history_data = []\n",
        "  test_history_data = []\n",
        "\n",
        "  with open(train_dataset_path, 'r', newline='') as csv_train:\n",
        "      csv_reader = csv.reader(csv_train)\n",
        "      header = next(csv_reader)  # 헤더 행, 실제론 data 바로 들어옴\n",
        "      header = [int(item) for item in header]\n",
        "      train_history_data.append(header)\n",
        "      for row in csv_reader:\n",
        "          r = [int(item) for item in row]\n",
        "          train_history_data.append(r)\n",
        "\n",
        "  with open(validation_dataset_path, 'r', newline='') as csv_validation:\n",
        "      csv_reader = csv.reader(csv_validation)\n",
        "      header = next(csv_reader)  # 헤더 행, 실제론 data 바로 들어옴\n",
        "      header = [int(item) for item in header]\n",
        "      validation_history_data.append(header)\n",
        "      for row in csv_reader:\n",
        "          r = [int(item) for item in row]\n",
        "          validation_history_data.append(r)\n",
        "\n",
        "  with open(test_dataset_path, 'r', newline='') as csv_test:\n",
        "      csv_reader = csv.reader(csv_test)\n",
        "      header = next(csv_reader)  # 헤더 행, 실제론 data 바로 들어옴\n",
        "      header = [int(item) for item in header]\n",
        "      test_history_data.append(header)\n",
        "      for row in csv_reader:\n",
        "          r = [int(item) for item in row]\n",
        "          test_history_data.append(r)\n",
        "\n",
        "  business_id_info = pd.read_csv('/content/business_info_in_philadelphia.csv', names = ['business_id', 'latitude', 'longitude', 'city'])\n",
        "  item_list = business_id_info['business_id'].tolist()\n",
        "\n",
        "  # print(\"여기는 함수한\")\n",
        "  # print(business_id_info)\n",
        "  # print(item_list)\n",
        "  # print(train_history_data[0])\n",
        "\n",
        "  num_users = len(train_history_data) # 필라델피아의 사용자 수 : 15919명\n",
        "  num_items = business_id_info.shape[0] # 필라델피아의 가게 수 : 14603개 , 마지막 business_id : 150336\n",
        "  return item_list, num_users, num_items, train_history_data, validation_history_data, test_history_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deyoQ4KUfdj6"
      },
      "outputs": [],
      "source": [
        "def get_train_data_user(item_list, num_users, train_list, num_negatives):\n",
        "    \"\"\"\n",
        "    학습데이터를 확인하고 num_negatives만큼의 negative데이터를 뽑아내 학습에 사용\n",
        "    negative sample\n",
        "    \"\"\"\n",
        "    num_items = len(item_list)\n",
        "    user_input, item_input, labels, batch_length = [],[],[],[]\n",
        "\n",
        "    for u in range(num_users):\n",
        "       np.random.shuffle(train_list[u])\n",
        "       if u == 0:\n",
        "           batch_length.append((1+num_negatives) * len(train_list[u]))\n",
        "       else:\n",
        "           batch_length.append((1+num_negatives) * len(train_list[u])+batch_length[u-1])\n",
        "\n",
        "       for i in train_list[u]:\n",
        "            # positive instance\n",
        "            user_input.append(u)\n",
        "            item_input.append(i)\n",
        "            labels.append(1)\n",
        "            # negative instances\n",
        "            for t in range(num_negatives):\n",
        "                j_idx = np.random.randint(0, num_items) # num_items == len(item_list), item_list에서 Negative sample을 골라야하기 때문에 인덱스를 골라줌\n",
        "                j = item_list[j_idx]\n",
        "                while j in train_list[u]:\n",
        "                    j_idx = np.random.randint(0, num_items)\n",
        "                    j = item_list[j_idx]\n",
        "                user_input.append(u)\n",
        "                item_input.append(j)\n",
        "                labels.append(0)\n",
        "    return user_input, item_input, labels, batch_length\n",
        "\n",
        "def get_input_data(batch_length, train_list, user_input, item_input, batch):\n",
        "    \"\"\"\n",
        "    recall, precision을 위한 입력 데이터 생성\n",
        "    \"\"\"\n",
        "    p=0\n",
        "    target_idx = []\n",
        "    if batch == 0:\n",
        "        p = 0\n",
        "    else:\n",
        "        p = batch_length[batch-1] # p : 지난 배치를 지나감\n",
        "\n",
        "    target_idx = item_input[p:batch_length[batch]]\n",
        "\n",
        "    temp = set(train_list[user_input[p]])\n",
        "\n",
        "    history = []\n",
        "    for idx in range(batch_length[batch]-p):\n",
        "        history.append(list(temp))\n",
        "    p=batch_length[batch]\n",
        "\n",
        "    return history, target_idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctZorxdFmCet"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNTxkywNgxGE"
      },
      "outputs": [],
      "source": [
        "def eval(model, num_users, testPositives, history_list, target_idx_list):\n",
        "    model.eval()\n",
        "    hit = [0,0,0,0,0]\n",
        "    precision = [0,0,0,0,0]\n",
        "    recall = [0,0,0,0,0]\n",
        "\n",
        "    k=[10,20,30,40,50]\n",
        "    a = 0.9\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for u in range(num_users):\n",
        "            count = 0\n",
        "            history = history_list[u]\n",
        "            target_idx = target_idx_list[u]\n",
        "            results=model(history, target_idx)\n",
        "\n",
        "            for i in range(len(k)):\n",
        "                topk = torch.topk(results,k[i])\n",
        "                topSet = set(target_idx[topk.indices.tolist()])\n",
        "\n",
        "                positives = set(testPositives[u])\n",
        "                count = len(topSet & positives)\n",
        "\n",
        "                precision[i] += (count/k[i])\n",
        "                recall[i] += (count/len(testPositives[u]))\n",
        "                if count >0:\n",
        "                    hit[i] += 1\n",
        "\n",
        "        for i in range(len(k)):\n",
        "            recall[i]/= num_users\n",
        "            precision[i]/= num_users\n",
        "            hit[i]/= num_users\n",
        "\n",
        "    return recall , precision, hit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klwMxeaGmPND"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7UhlqAISMxx"
      },
      "outputs": [],
      "source": [
        "def train(model, epoch, history, user_input, item_input, labels, batch_length, optimizer, validation_history_data, user_input_validation, item_input_validation, labels_validation, batch_length_validation, test_history_data):\n",
        "    loss_func = nn.BCELoss()\n",
        "    before_loss = 210000000.0\n",
        "    max_re = 0.0\n",
        "\n",
        "    recall = []\n",
        "    prec = []\n",
        "    hit = []\n",
        "\n",
        "    for e in range(epoch):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        start_time = int(time.time())\n",
        "\n",
        "        p = 0 # 지난 배치는 지나가도록 확인해주는 변수\n",
        "\n",
        "        for b in range(len(batch_length)): # user 수, 즉 배치 개수 만큼 반복\n",
        "          print(f\"User : {b}\")\n",
        "          batch_size = batch_length[b]\n",
        "          user = []\n",
        "          for i in range(5): # 5 : negative_num + 1\n",
        "            user.extend(history[b])\n",
        "\n",
        "          #print(f\"user_len : {len(user)}\")\n",
        "          #print(f\"p: {p}, batch_size : {batch_size}\")\n",
        "          item = item_input[p:batch_size]\n",
        "\n",
        "          #print(f\"len_item : {len(item)}\")\n",
        "          optimizer.zero_grad()\n",
        "          prediction = model(user, item)\n",
        "          temp = torch.tensor(labels[p:batch_size], dtype=torch.float32)\n",
        "\n",
        "          #print(prediction)\n",
        "          #print(temp)\n",
        "          loss = model.loss_func(prediction,temp)\n",
        "\n",
        "          p = batch_size # p 업데이트\n",
        "          loss.backward()\n",
        "          train_loss += loss.item()\n",
        "          optimizer.step()\n",
        "\n",
        "        end_time = int(time.time())\n",
        "\n",
        "        print(\"Train Epoch: {}; time: {} sec; loss: {:.4f}\".format(e+1, end_time-start_time,train_loss))\n",
        "\n",
        "        re,pr,hi = eval(model, num_users, validation_history_data, train_history_data, item_input_validation)\n",
        "        recall.append(re)\n",
        "        prec.append(pr)\n",
        "        hit.append(hi)\n",
        "\n",
        "        print(\"recall: {} precision: {} hit: {};\".format( re[0],pr[0],hi[0]))\n",
        "        re,pr,hi,re_d,pr_d,hi_d = eval(model,num_users, test_history_data, train_history_data, item_input_test)\n",
        "\n",
        "        recall.append(re)\n",
        "        prec.append(pr)\n",
        "        hit.append(hi)\n",
        "\n",
        "\n",
        "        end_time = int(time.time())\n",
        "        print(\"time: {} sec;\".format( end_time-start_time))\n",
        "\n",
        "    return recall , prec, hit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI9LixjzmEVl"
      },
      "source": [
        "## NAIS Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glIwKYS8tr8T"
      },
      "outputs": [],
      "source": [
        "class NAIS_basic(nn.Module):\n",
        "    def __init__(self, item_num, embed_size, beta): # embed_size : 64, beta : 0.5\n",
        "        super(NAIS_basic, self).__init__()\n",
        "        self.embed_size = embed_size # concat 연산 시 * 2\n",
        "        self.item_num = item_num\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embed_history = nn.Embedding(item_num, embed_size) # (m:14603 * d:64), 과거 방문한 데이터(q), 유저별로 각각 하나씩 가져야하나 ?\n",
        "        self.embed_target = nn.Embedding(item_num, embed_size) # (m:14603 * d:64), 예측 데이터(p)\n",
        "\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.loss_func = nn.BCELoss() # binary cross entropy\n",
        "\n",
        "        # Attention을 위한 MLP Layer 생성\n",
        "        self.attn_layer1 = nn.Linear(embed_size, embed_size)\n",
        "        self.attn_layer2 = nn.Linear(embed_size, 1, bias = False)\n",
        "\n",
        "        self._init_weight_()\n",
        "\n",
        "    def _init_weight_(self):\n",
        "        # weight 초기화, 표준편차 : 0.01\n",
        "        nn.init.normal_(self.embed_history.weight, std=0.01)\n",
        "        nn.init.normal_(self.embed_target.weight, std=0.01)\n",
        "\n",
        "        # bias 초기화\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, history, target):\n",
        "        #배치 사이즈만큼 잘라서 넣어줌\n",
        "        #print(len(history), len(target))\n",
        "        history_tensor = torch.LongTensor(history)\n",
        "        target_tensor = torch.LongTensor(target)\n",
        "        #print(f\"history_tensor : {history_tensor.shape}\")\n",
        "        #print(f\"target_tensor : {target_tensor.shape}\")\n",
        "\n",
        "        prediction = self.attention_network(history_tensor,target_tensor)\n",
        "        return self.sigmoid(prediction)\n",
        "\n",
        "    def attention_network(self, user_history, target_item):\n",
        "        \"\"\"\n",
        "        b: batch size (= input_item_num)\n",
        "        h: history size (h * 5 = item_num = batch_size)\n",
        "        d: embedding size\n",
        "        \"\"\"\n",
        "        #print(\"#### def attention network start #####\")\n",
        "        #print(len(user_history), user_history)\n",
        "        #print(len(target_item), target_item)\n",
        "\n",
        "        history = self.embed_history(user_history) # (b * d)\n",
        "        target = self.embed_target(target_item) # (b * d)\n",
        "        batch_dim = len(target) # (b)\n",
        "\n",
        "        #print(f\"history_embedding : {history.shape}\")\n",
        "        #print(f\"target_embedding : {target.shape}\")\n",
        "        #print(f\"batch_dim : {batch_dim}\")\n",
        "\n",
        "        target = torch.reshape(target,(batch_dim, 1,-1))\n",
        "        #print(f\"target_reshaped to batch_dim : {target.shape}\") # (b * 1 * d)\n",
        "\n",
        "        input = history * target # (b*d) * (b*1*d) => (b * b * d)\n",
        "        #print(f\"input_data : {input.shape}\")\n",
        "\n",
        "        #print(\"#### attention layer start\")\n",
        "        attention_result = self.relu(self.attn_layer1(input)) # (b * b * d)\n",
        "        #print(f\"first attention : {attention_result.shape}\")\n",
        "\n",
        "        attention_result = self.attn_layer2(attention_result) # (b * b * 1)\n",
        "        #print(f\"second attention : {attention_result.shape}\")\n",
        "\n",
        "        exp_A = torch.exp(attention_result) # (b * b * 1)\n",
        "        #print(f\"exp_A : {exp_A.shape}\")\n",
        "        exp_A = exp_A.squeeze(dim=-1)# (b * b)\n",
        "        #print(f\"exp_A_Squeeze : {exp_A.shape}\")\n",
        "\n",
        "        mask = self.get_mask(user_history,target_item) # (b * b)\n",
        "        #print(f\"mask : {mask.shape}\")\n",
        "        exp_A = exp_A * mask # (b * b)\n",
        "        #print(f\"exp_A_mask : {exp_A.shape}\")\n",
        "        exp_sum = torch.sum(exp_A,dim=-1) # (b)\n",
        "        #print(f\"exp_A_sum : {exp_sum.shape}\")\n",
        "        exp_sum = torch.pow(exp_sum, self.beta) # (b)\n",
        "        #print(f\"exp_A_pow : {exp_sum.shape}\")\n",
        "\n",
        "        attn_weights = torch.divide(exp_A.T,exp_sum).T # (b * b)\n",
        "        #print(f\"attn_weights = {attn_weights.shape}\")\n",
        "        attn_weights = attn_weights.reshape([batch_dim,-1, 1])# (b * b * 1)\n",
        "        #print(f\"attn_weights_reshaped = {attn_weights.shape}\")\n",
        "        result = history * attn_weights# (b * b * d)\n",
        "        target = target.reshape([batch_dim,-1,1]) # (b * d * 1)\n",
        "\n",
        "\n",
        "        prediction = torch.bmm(result, target).squeeze(dim=-1) # (b * b * 1) -> (b * b)\n",
        "        prediction = torch.sum(prediction, dim = -1) # (b)\n",
        "        #print(f\"#### return prediction{prediction.shape}\")\n",
        "        return prediction\n",
        "\n",
        "    def get_mask(self, user_history, target_item):\n",
        "        target_item = target_item.reshape([len(target_item),1])\n",
        "        mask = user_history != target_item\n",
        "        return mask\n",
        "    def loss_function(self, prediction, label):\n",
        "        return self.loss_func(prediction, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS89rjTrmQhj"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "CGUAkoS3dlFr",
        "outputId": "042b62e1-1c4f-4a04-e1a8-967076caef8a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-48380393545c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnum_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mitem_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_history_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_history_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0muser_input_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_input_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_length_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_negatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0muser_input_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_input_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_length_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_history_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_negatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-5370aead1fba>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_reader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 헤더 행, 실제론 data 바로 들어옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mtrain_history_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-5370aead1fba>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_reader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 헤더 행, 실제론 data 바로 들어옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mtrain_history_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # USE_CUDA = torch.cuda.is_available()\n",
        "    # DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "    #print(\"사용하는 Device : \", DEVICE)\n",
        "    epoch = 40\n",
        "    num_negatives = 4\n",
        "\n",
        "    item_list, num_users, num_items, train_history_data, validation_history_data, test_history_data = load_data()\n",
        "    user_input_train, item_input_train, labels_train, batch_length_train = get_train_data_user(item_list, num_users, train_history_data, num_negatives)\n",
        "    user_input_validation, item_input_validation, labels_validation, batch_length_validation = get_train_data_user(item_list, num_users, validation_history_data, num_negatives)\n",
        "    user_input_test, item_input_test, labels_test, batchlength_test = get_train_data_user(item_list, num_users, test_history_data, num_negatives)\n",
        "    print(num_items, num_users)\n",
        "    print(len(item_list), item_list[0], item_list[-1])\n",
        "\n",
        "\n",
        "    # for i in range(5):\n",
        "    #     nais_model_name = \"NAIS_basic_product\"\n",
        "\n",
        "    #     nais_model = NAIS_basic(num_items, 64, 0.5) # num_items, embed_size, beta\n",
        "    #     optimizer1 = optim.Adagrad(nais_model.parameters(), lr=0.005, weight_decay=0.001)\n",
        "    #     print(nais_model_name + \" model\")\n",
        "\n",
        "    #     recall, prec, hit, recall_d, prec_d, hit_d = train(nais_model, epoch, train_history_data, user_input_train, item_input_train, labels_train, batch_length_train, optimizer1, validation_history_data, user_input_validation, item_input_validation, labels_validation, batch_length_validation, test_history_data)\n",
        "\n",
        "    #     with open(\"/content/result/\"+nais_model_name+ str(i)+\".txt\", mode='wt') as f:\n",
        "    #       for g in range(len(recall)):\n",
        "    #           f.write(\"epoch: \"+str((g+1))+\"-----------------------------------------\\n\")\n",
        "    #           f.write(\"recall:\"+ str( recall[g])+\", precision: \"+ str(prec[g])+\", hit: \"+ str(hit[g])+\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPU7m/jSsfoAmowpJE+SAqN",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
